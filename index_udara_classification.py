# -*- coding: utf-8 -*-
"""index_udara_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQnwExN0uDZA2ba1i63u68SIujeU7OSc

# Proyek Predictive Analytics : Klasifikasi Pencemaran Udara Jakarta

Nama  : Paramita Citra Indah Mulia

Email : paramitamulia@gmail.com

# Import library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from pprint import pprint

"""# Data Understanding"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/combined.csv')
data.head()

data.tail()

print(data.shape)

data.info()

"""Fitur-fitur pada data:

1. tanggal : Tanggal pengukuran kualitas udara
2. stasiun : Lokasi pengukuran di stasiun
3. pm10 : Partikulat salah satu parameter yang diukur
4. pm25 : Partikulat salah satu parameter yang diukur
5. so2 : Sulfida (dalam bentuk SO2) salah satu parameter yang diukur
6. co : Carbon Monoksida salah satu parameter yand diukur
7. o3 : Ozon salah satu parameter yang diukur
8. no2 : NItrogen dioksida salah satu parameter yang diukur
9. max : Nilai ukur paling tinggi dari seluruh parameter yang diukur dalam waktu yang sama
10. critical : Parameter yang hasil pengukurannya paling tinggi
11. categori : Kategori hasil perhitungan indeks standar pencemaran udara

**EDA**

1. Jumlah data per kategori
"""

plt.figure(figsize=(10, 8))
ax = sns.countplot(x='categori', data=data)

for p in ax.patches:
    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

plt.show()

"""terdapat 5 kategori yaitu sedang, baik, tidak sehat, sangat tidak sehat, dan tidak ada data. Kategori 'tidak ada data' akan dihapus, dan karena jumlah data dengan kategori sangat tidak sehat hanya 3, maka pada klasifikasi ini hanya akan digunakan tiga kategori saja yaitu sedang, baik, dan tidak sehat.

2. Persebaran Kategori Kualitas Udara di 5 Stasiun
"""

data['stasiun'].value_counts()

"""Data terbanyak yaitu masing-masing 700 data adalah data dari stasiun DKI1 (Bunderan HI), DKI5 (Kebon Jeruk) Jakarta Barat, DKI4 (Lubang Buaya), DKI3 (Jagakarsa), DKI2 (Kelapa Gading). Sedangkan data paling sedikit dengan jumlah hanya 1 data berasal dari stasiun dengan kode 63, 57, 24, 64, 73, 89, 101, 60, 41, 76, 61, dan 25."""

#Persebaran Kategori Kualitas Udara di 5 Stasiun Terbanyak
selected_stations = ["DKI1 (Bunderan HI)", "DKI5 (Kebon Jeruk) Jakarta Barat", "DKI4 (Lubang Buaya)", "DKI3 (Jagakarsa)", "DKI2 (Kelapa Gading)"]
filtered_data = data[data['stasiun'].isin(selected_stations)]

catplot = sns.catplot(x='stasiun', hue='categori', kind='count', data=filtered_data)
catplot.fig.set_size_inches(18, 5)

for ax in catplot.axes.flat:
    for bar in ax.patches:
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 2,
            round(bar.get_height(), 2),
            ha='center',
            va='bottom'
        )

plt.title('Persebaran Kategori Kualitas Udara di 5 Stasiun')
plt.show()

"""Stasiun Lubang Buaya merupakan daerah dengan indeks udara tidak sehat terbanyak dan Bunderan HI adalah yang paling sedikit.

Sementara itu, seluruh stasiun memiliki kategori sedang yang hampir sama dengan terbanyak adalah stasiun Jagakarsa
"""

plt.figure(figsize=(12, 6))
ax = sns.countplot(x='critical', data=data)

for p in ax.patches:
    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

plt.show()

"""Parameter dengan hasil pengukuran tertinggi terbanyak adalah PM25. Menurut Kementerian Lingkungan Hidup dan Kehutanan, parameter PM2,5 merupakan parameter pencemar udara paling berpengaruh terhadap kesehatan manusia.

# Data Preparation
"""

#definisi data dengan 3 kategori yang telah ditetapkan
data2 = data[~data['categori'].isin(['SANGAT TIDAK SEHAT', 'TIDAK ADA DATA'])]

data2.info()

"""Data pm25 hanya sejumlah 1749 dari 3620 data,
berdasarkan domain knowledge bahwa pm25 merupakan parameter pencemar udara paling berpengaruh terhadap kesehatan, data yang akan digunakan adalah data yang memiliki pm25 non-nul.
"""

data_br = data2[data2['pm25'].notnull()]
data_br.info()

data_br.isnull().sum()

"""melihat apakah masih ada missing value berbentuk lain"""

cols_with_nan_or_dash = data_br.columns[data_br.isin([np.nan, '---']).any()].tolist()
print(cols_with_nan_or_dash)

"""Terdapat missing value pada variabel 'pm10', 'so2', 'co', 'o3', 'no2', 'pm25'.
Pada variabel numerik ini, mv akan diisi dengan nilai mean.
"""

# Mengganti '---' dengan NaN
data_br = data_br.replace('---', np.nan)

# Mengisi NaN dengan mean pada kolom numerik
for kolom in ['pm10', 'so2', 'co', 'o3', 'no2', 'max', 'pm25']:
    data_br[kolom] = data_br[kolom].astype(float).fillna(data_br[kolom].astype(float).mean())

columns_with_dash = [col for col in data_br.columns if (data_br[col] == '---').any()]
print(columns_with_dash)

"""cek data duplikat"""

data_br.drop('tanggal', axis=1, inplace=True)

data_br.duplicated().sum()

"""tidak ada data duplikat, missing value dan data duplikat berhasil ditangani."""

#melihat jumlah data pada masing-masing kategori setelah dilakukan preprocessing
#melihat jumlah categori
plt.figure(figsize=(6, 4))
ax = sns.countplot(x='categori', data=data_br)

# Tambahkan jumlah data di atas setiap bar
for p in ax.patches:
    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

plt.show()

"""melihat jumlah data yang ada, akan digunakan 200 data kategori sedang, 149 kategori baik, dan 200 data kategori tidak sehat."""

# Mengambil 200 data dengan kategori 'sedang'
df_sedang = data_br[data_br['categori'] == 'SEDANG'].head(200)

# Mengambil semua data dengan kategori 'baik'
df_baik = data_br[data_br['categori'] == 'BAIK']

# Mengambil 200 data dengan kategori 'tidak sehat'
df_tidak_sehat = data_br[data_br['categori'] == 'TIDAK SEHAT'].head(200)

# Menggabungkan ketiga DataFrame tersebut
df_mod = pd.concat([df_sedang, df_baik, df_tidak_sehat])

#melihat jumlah data pada masing-masing kategori setelah dilakukan preprocessing
#melihat jumlah categori
plt.figure(figsize=(6, 4))
ax = sns.countplot(x='categori', data=df_mod)

# Tambahkan jumlah data di atas setiap bar
for p in ax.patches:
    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

plt.show()

df_mod.info()

"""kolom critical dan categori masih dalam bentuk objek. pada kedua kolom akan dilakukan pelabelan dengan LabelEncoder."""

#pelabelan dengan label encoder
label_mappings = {}
le = LabelEncoder()

for column in ['stasiun', 'critical', 'categori']:
    df_mod[column] = le.fit_transform(df_mod[column])

    label_mappings[column] = dict(zip(le.classes_, range(len(le.classes_))))

pprint(label_mappings)

df_mod.head(5)

#normalisasi data
standard_scaler = StandardScaler()
df_standard = standard_scaler.fit_transform(df_mod)
df_standard

# Membagi data menjadi training set dan test set
X = df_mod[['stasiun', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'pm25', 'critical']]
y = df_mod['categori']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Model Development

Algoritma 1 : Random Forest
"""

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_predictions = rf.predict(X_test)
print("Akurasi Random Forest:", accuracy_score(y_test, rf_predictions))

"""Algoritma 2 : SVM"""

# KNN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_predictions = knn.predict(X_test)
print("Akurasi KNN:", accuracy_score(y_test, knn_predictions))

"""# Evaluation"""

# Buat dataframe berisikan score masing-masing algoritma dari data train dan test
score = pd.DataFrame()

# buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'RF': rf, 'KNN': knn}

# hitung f1-score masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
  score.loc[name, 'train'] = f1_score(y_train, y_pred=model.predict(X_train), average='weighted')
  score.loc[name, 'test'] = f1_score(y_test, y_pred=model.predict(X_test), average='weighted')

# Panggil score
score

# hitung confusion matrix dari data testing
cnf_matrix = confusion_matrix(y_test, rf_predictions)

# Plot confusion matrix
sns.heatmap(cnf_matrix, annot=True, cbar=False, cmap='Blues')
plt.title('Confusion Matrix RF', fontsize=15)
plt.show()

print(classification_report(y_test, rf_predictions))

# hitung confusion matrix dari data testing
cnf_matrix2 = confusion_matrix(y_test, knn_predictions)

# Plot confusion matrix
sns.heatmap(cnf_matrix2, annot=True, cbar=False, cmap='Blues')
plt.title('Confusion Matrix KNN', fontsize=15)
plt.show()

print(classification_report(y_test, knn_predictions))

prediksi = X_test[:5].copy()
pred_dict = {'y_true':y_test[:5]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)